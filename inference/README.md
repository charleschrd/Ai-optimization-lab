# ðŸ“ Inference â€” Text Generation with Tinyâ€‘GPTâ€‘2

This script lets you generate text locally from the fineâ€‘tuned model saved in `training/tiny-gpt2-finetuned/`.

---

## 1Â Â· Quickâ€¯Start (Docker)

```bash
docker run --rm -it --gpus all -e MODE=shell -v $(pwd):/workspace ai-opt-lab
# inside container
python inference/generate.py \
  --prompt "Il Ã©tait une fois" \
  --max_tokens 80 \
  --temperature 0.8 \
  --do_sample
````

---

## 2Â Â· CLI Options

| Flag            | Default    | Description                      |
| --------------- | ---------- | -------------------------------- |
| `--prompt`      | *required* | Initial text to continue         |
| `--max_tokens`  | 50         | Number of tokens to generate     |
| `--temperature` | 1.0        | >1â€¯=â€¯creative, <1â€¯=â€¯conservative |
| `--top_p`       | 0.9        | Nucleus sampling                 |
| `--do_sample`   | False      | If false â†’ greedy decoding       |
| `--out_file`    | *None*     | Append output to file            |

Example:

```bash
python inference/generate.py --prompt "Once upon a time" --max_tokens 60 --temperature 0.7 --out_file outputs.txt
```

---

## ðŸ‡«ðŸ‡·Â RÃ©sumÃ© rapide

* Lancer un shell Docker puisÂ :

  ```bash
  python inference/generate.py --prompt "Bonjour" --max_tokens 60 --do_sample
  ```
* ParamÃ¨tres modifiablesâ€¯: longueur, tempÃ©rature, topâ€‘p, etc.
* Ajoutez `--out_file` pour sauvegarder chaque sortie.

````

---
