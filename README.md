# ðŸ§  AI Optimization Lab

This repository is a personal lab designed to experiment with the full lifecycle of modern language models (LLMs) â€” from training to hardware-level optimization.

## ðŸŽ¯ Goals

- Train a small-scale LLM on a custom corpus
- Apply model **pruning**, **distillation**, and **quantization**
- Simulate hardware deployment via **FPGA (HLS)** using Vitis AI or HLS4ML
- Visualize performance trade-offs: size, speed, accuracy, memory usage

## ðŸ“¦ Structure

ai-optimization-lab/
â”œâ”€â”€ data/ # datasets and preprocessing
â”œâ”€â”€ training/ # training scripts (TinyGPT, etc.)
â”œâ”€â”€ pruning/ # weight pruning experiments
â”œâ”€â”€ distillation/ # student-teacher models
â”œâ”€â”€ quantization/ # int8/fp16 quantization
â”œâ”€â”€ simulation_fpga/ # Vivado/HLS4ML integration
â”œâ”€â”€ notebooks/ # Jupyter analysis
â”œâ”€â”€ utils/ # shared helpers
â”œâ”€â”€ Dockerfile # reproducible environment
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

markdown
Copier
Modifier

## ðŸ§ª Future Work

- Add CI pipeline (pytest + pre-commit)
- Add HuggingFace Spaces integration
- Setup ArgoCD for MLOps demo
