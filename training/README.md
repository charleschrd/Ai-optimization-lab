# Training Log â€” TinyGPT

This is a snapshot of a successful training session performing on a local VM using GPU passthrough.

![Training in progress](./Training.png)

---

## 1Â Â· QuickÂ Start (inside Docker)

### Oneâ€‘Shot Training
```bash
docker run --rm --gpus all -e MODE=train -v $(pwd):/workspace ai-opt-lab
```

---

## 2Â Â· Script overview (`train_tiny_gpt.py`)

| Step              | What happens                                                                       |
| ----------------- | ---------------------------------------------------------------------------------- |
| **Load model**    | `distilgpt2` from HuggingFace Hub                                                  |
| **Fix padÂ token** | If missing, set `tokenizer.pad_token = tokenizer.eos_token` (avoids padding error) |
| **Dataset**       | DefaultÂ =Â `wikitextâ€‘2-raw-v1`; switch to TinyStories by editing the script         |
| **TrainingArgs**  | 3Â epochs Â· batchÂ 8 Â· LRÂ 2eâ€‘5 Â· logsÂ â†’Â `./logs`                                     |
| **TensorBoard**   | `report_to="tensorboard"` â€” run `tensorboard --logdir ./logs`                      |
| **Output**        | Final model in `training/tiny-gpt2-finetuned/`                                     |

---

## 3Â Â· Monitoring with TensorBoard

```bash
tensorboard --logdir training/logs --port 6006
# open http://localhost:6006
```

---

## 4Â Â· Switching dataset

```python
# in train_tiny_gpt.py
dataset = load_dataset("roneneldan/TinyStories", "all")
```

---

## 5Â Â· Environment

* GPUÂ : RTXâ€¯3070 passthrough
* CUDAÂ 12.1 Â· PyTorchâ€¯2.2 Â· Transformersâ€¯4.42

---

## ðŸ‡«ðŸ‡·Â RÃ©sumÃ© rapide

* EntraÃ®nementâ€¯:

  ```bash
  docker run --rm --gpus all -e MODE=train -v $(pwd):/workspace ai-opt-lab
  ```
* Visualisationâ€¯:

  ```bash
  tensorboard --logdir training/logs
  ```
* Modifiez le dataset/hyperparams dans le script puis relancez.
